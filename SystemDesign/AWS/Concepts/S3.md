# S3

- Object-level storage
- Designed for 11 9s durability (e.g. 99.9999999%)
- Event trigger


bucket is no a folder, it's more like a domain in the url

- owenr of the bucket is the only one who has all acess to everything in bucket.
- the only way to have someone to acess the bucket is to either
  - make the bucket public
  - using policy to provide controlled access

Use case:
- S3 can also be used as an origin for a content delivery network (CDN) such a Amazon CloudFront
- S3 can host entire static websites
- S3 can be used as backup for EC2 etc.
- use S3 as a data store for computation or large-scale analytics, such as finanicial transaction analysis, clickstream analytics, and media transcoding. Amazon S3 can support these workloads becuase of its horizontal scaling ability, which easily allows multiple concurrent transctions. 

Versioning-enabled bucket can provide version control.
- without using version, if you send a delete request, the object would be deleted
- but by using version, you can delete only a version
Consider using S3 object lock for data retention or protection.


S3 can also use CORS to do access control.


all 3 ways to access AWS sercice (console / CLI / SDKs) are all through the same set of API.

When uploading large file to S3, S3 would chunk it (no matter what size it is) and upload each chunk in parallel with different channels to different nodes, when the upload is done, the API would return response that it is done.  
S3 supports resume if one upload is interrupted.  

With Amazon transfer acceleration, you can upload S3 much faster.  
--> Amazon CloudFront would look for an edge location and let you upload to a closest S3 node for you


Anything faster than S3?
- AWS Snowball is a hardware box to support petabyte-scale data transport, and then mail this one (some human can drive the AWS Snowmobile) to the your data center and plugin for you.
On the Snowball box, there is an embedded kindle to record some labels for you.  


Good use case for S3:  
1. when you need to write once, read many times
2. Spiky data access
3. Large number of users and diverse amounts of content
4. Growing data sets

Not ideal use case for S3:
1. Block storage requiremtns - write many times
  - e.g. update constantly lik Block storage requiremtns
    - a large data file and update individual pieces or individual characters frequently
    - rolling log files, adding one line a time
    - database file, constantly update little bit of chunk
  - these are good for block storage, but not what S3 is engineered around
2. Frequently changing data
3. Long-term archival storage


Paying for S3
- GBs per month
- transer OUT to other regions or the internet
- PUT, COPY, POST, LIST, and GET requests

You do NOT have to pay for
- transfer IN to S3
- Transfer OUT to Amazon EC2 in the same region, or to CloudFront

# S3 Glacier

Glacier is design to be extremely low cost for long-term archiaved data

use case:
- nobody read data from Glacier
- when there is a request to get data from Glacier, people would "see" it and then put a request to S3...


Audit.log --> Audit Archive --> audit Vault <-- Vault Lock
e.g.
- financial institution
- ICC org


There are multiple tiering S3 can support
1. S3 Standard -- general purpose
2. S3 Standard IA -- infrequent but rapid accss
3. S3 One Zone IA -- re-createable, infrequently acessed data
4. Amazon Glacier / Deep Archive -- archival data, cheapest available storage tier


AWS uses ml to detech which tier is good for your data based on the visiting frequentcy, you can also customize the Lifecycle policies by yourselves:
e.g.  
S3 Standard --> <30 days> --> S3 standard (infrequent acess) --> <60 days> --> Amazon Glaciers --> <365 Days> --> Delete


 
## How to choose which region?

1. Are there relevant region "data privacy" laws?
2. Can customer data be stored outside the country?
3. Can you meet your goverance obligations

Proximity of users to data
- choose the region cloest to your users


Services and feature availability
- some services not yet avaibale in all regions
- Can use some services cross-region, but at increased latency
- service expanded to new regions regularly


Cos Effectiveness
- costs vary by region
- some service like Amazon S3 have costs to transferring data out
- consider the cost-effectiveness of replicating theentirre environement in another region

## Lab1: hosting a static webite - 20min

- crate an s3 bucket
- deploy your website
- make your site publicly available

NOTE:
1. make sure the bucket is public accessible by deselect "block all public access"
2. Go to S3 --> properties --> static website hosting 
  - enable static website hosting
3. objects stored in S3 are private by default, need to make sure uploaded objects publicly accessible
  - there are 2 ways to make it public
    - using bucket policy to make an entire bucket or just a directory within a bucket public
    - you can also use accss control list (ACL) to make individual bucket objects public
  - it is normally safer to make individualobjects public because this avoids other objects being accidentally made public
    - select all files, then "Actions" --> "Make public"


## Q & A

- when using S3, which of the following do you pay for?
  - GBs per month
  - transfer OUT to other regions or the Internet
  - PUT, COPY, POST, LIST, and GET requests
- "Versioning" enabled S3 buckets enable you to recover objects from accidental deleation or overwrite
- A bucket "policy" can be used to enabled anc control acess to a bucket's objects by other AWS accounts and users
- A "lifecycle" policy can be used to automatically move objects from hot to cold storage after specified lengths of time
- 








  

